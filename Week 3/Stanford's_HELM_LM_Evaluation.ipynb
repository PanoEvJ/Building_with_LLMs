{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "A100"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### Stanford's Heuristic Evaluation of Language Models\n",
        "\n",
        "Based on the work in this [repository](https://github.com/stanford-crfm/helm), we'll be implementing HELM to evaluate a model today!"
      ],
      "metadata": {
        "id": "hcPOD7h1s68l"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "As always, let's grab some dependencies! \n",
        "\n",
        "**PLEASE RESTART YOUR ENV AFTER INSTALLING THESE DEPENDENCIES**"
      ],
      "metadata": {
        "id": "1XneqmcEtpRe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q crfm-helm\n",
        "!pip install -q typing_extensions==4.5.0"
      ],
      "metadata": {
        "id": "vAbfmcNjtsPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -U torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
      ],
      "metadata": {
        "id": "HgAjBesZyZJx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, let's dive in to doing a simple evaluation!"
      ],
      "metadata": {
        "id": "dckk0SXyweMm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!echo 'entries: [{description: \"mmlu:subject=philosophy,model=huggingface/gpt2\", priority: 1}]' > run_specs.conf"
      ],
      "metadata": {
        "id": "3Li97dKSttn-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!helm-run --conf-paths run_specs.conf --local --max-eval-instances 1 --suite v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BFK0KChJwlI-",
        "outputId": "8385bfce-0e7b-409f-c436-0f0de4b02348"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-30 22:38:03.415059: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-30 22:38:03.468402: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-30 22:38:04.369378: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "main {\n",
            "  Read 1 run entries from run_specs.conf\n",
            "  1 entries produced 1 run specs\n",
            "  run_specs {\n",
            "    RunSpec(name='mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2', scenario_spec=ScenarioSpec(class_name='helm.benchmark.scenarios.mmlu_scenario.MMLUScenario', args={'subject': 'philosophy'}), adapter_spec=AdapterSpec(method='multiple_choice_joint', global_prefix='', instructions='The following are multiple choice questions (with answers) about philosophy.\\n', input_prefix='Question: ', input_suffix='\\n', reference_prefix='A. ', reference_suffix='\\n', output_prefix='Answer: ', output_suffix='\\n', instance_prefix='\\n', substitutions=[], max_train_instances=5, max_eval_instances=1, num_outputs=5, num_train_trials=1, sample_train=True, model='huggingface/gpt2', temperature=0.0, max_tokens=5, stop_sequences=['\\n'], random=None), metric_specs=[MetricSpec(class_name='helm.benchmark.basic_metrics.BasicMetric', args={'names': ['exact_match', 'quasi_exact_match', 'prefix_exact_match', 'quasi_prefix_exact_match']})], data_augmenter_spec=DataAugmenterSpec(perturbation_specs=[], should_augment_train_instances=False, should_include_original_train=False, should_skip_unchanged_train=False, should_augment_eval_instances=False, should_include_original_eval=False, should_skip_unchanged_eval=False, seeds_per_instance=1), groups=['mmlu'])\n",
            "  } [0.0s]\n",
            "  Running locally in root mode with local path: prod_env\n",
            "  Created cache with config: SqliteCacheConfig(path='prod_env/cache/huggingface.sqlite')\n",
            "  AutoClient: cache_path = prod_env/cache\n",
            "  AutoClient: mongo_uri = \n",
            "  Found 1 account(s).\n",
            "  Created cache with config: SqliteCacheConfig(path='prod_env/cache/perspectiveapi.sqlite')\n",
            "  0% 0/1 [00:00<?, ?it/s]  Running mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 {\n",
            "    scenario.get_instances {\n",
            "      ensure_file_downloaded {\n",
            "        Not downloading https://people.eecs.berkeley.edu/~hendrycks/data.tar because benchmark_output/scenarios/mmlu/data already exists\n",
            "      } [0.0s]\n",
            "      benchmark_output/scenarios/mmlu/data/auxiliary_train/philosophy_auxiliary_train.csv doesn't exist, skipping\n",
            "      Reading benchmark_output/scenarios/mmlu/data/dev/philosophy_dev.csv\n",
            "      Reading benchmark_output/scenarios/mmlu/data/val/philosophy_val.csv\n",
            "      Reading benchmark_output/scenarios/mmlu/data/test/philosophy_test.csv\n",
            "    } [0.005s]\n",
            "    350 instances, 5 train instances, 1/345 eval instances\n",
            "    DataPreprocessor.preprocess {\n",
            "    } [0.0s]\n",
            "    MultipleChoiceJointAdapter.adapt {\n",
            "      6 instances, choosing 5/5 train instances, 1 eval instances\n",
            "      Adapting with train_trial_index=0 {\n",
            "        Sampled 5 examples for trial #0.\n",
            "        Parallelizing computation on 1 items over 4 threads {\n",
            "          Created cache with config: SqliteCacheConfig(path='prod_env/cache/huggingface.sqlite')\n",
            "          Loading huggingface/gpt2 with Hugging Face Transformers {\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A          } [0.07s]\n",
            "100% 1/1 [00:00<00:00, 13.69it/s]\n",
            "        } [0.074s]\n",
            "        Sample prompts {\n",
            "          reference index = None, request_mode = None {\n",
            "            The following are multiple choice questions (with answers) about philosophy.\n",
            "            \n",
            "            Question: The study of reality in the broadest sense, an inquiry into the elemental nature of the universe and the things in it, is known as _____.\n",
            "            A. metaphysics\n",
            "            B. epistemology\n",
            "            C. quantum physics\n",
            "            D. axiology\n",
            "            Answer: A\n",
            "            \n",
            "            Question: According to Moore’s “ideal utilitarianism,” the right action is the one that brings about the greatest amount of:\n",
            "            A. pleasure.\n",
            "            B. happiness.\n",
            "            C. good.\n",
            "            D. virtue.\n",
            "            Answer: C\n",
            "            \n",
            "            Question: Psychological egoism is:\n",
            "            A. an ethical theory about how we ought to behave.\n",
            "            B. a generalization concerning the way people tend to behave.\n",
            "            C. a claim about human nature and the ways people are capable of behaving.\n",
            "            D. none of the above.\n",
            "            Answer: C\n",
            "            \n",
            "            Question: Before Tolstoy's Christian conversion, what was his perspective on the meaning of life?\n",
            "            A. optimist\n",
            "            B. satisfied\n",
            "            C. nominally religious\n",
            "            D. pessimist\n",
            "            Answer: D\n",
            "            \n",
            "            Question: According to d'Holbach, people always act according to _____.\n",
            "            A. free choices\n",
            "            B. dictates of the soul\n",
            "            C. necessary natural laws\n",
            "            D. undetermined will\n",
            "            Answer: C\n",
            "            \n",
            "            Question: What does the notion of “meaning in life” refer to?\n",
            "            A. external meaning\n",
            "            B. god's plan\n",
            "            C. internalmeaning\n",
            "            D. meaning in an afterlife\n",
            "            Answer:\n",
            "          } [0.0s]\n",
            "        } [0.0s]\n",
            "      } [0.074s]\n",
            "      1 requests\n",
            "    } [0.074s]\n",
            "    Executor.execute {\n",
            "      Parallelizing computation on 1 items over 4 threads {\n",
            "        CUDA is available, initializing with a GPU...\n",
            "        Loading Hugging Face model for config gpt2 {\n",
            "\n",
            "  0% 0/1 [00:00<?, ?it/s]\u001b[A        } [3.68s]\n",
            "        Loading Hugging Face tokenizer model for config gpt2 {\n",
            "        } [0.249s]\n",
            "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n",
            "\n",
            "100% 1/1 [00:05<00:00,  5.00s/it]\n",
            "      } [5.005s]\n",
            "      Processed 1 requests\n",
            "    } [5.005s]\n",
            "    2 metrics {\n",
            "      BasicMetric(exact_match,quasi_exact_match,prefix_exact_match,quasi_prefix_exact_match) {\n",
            "        Parallelizing computation on 1 items over 4 threads {\n",
            "\n",
            "100% 1/1 [00:00<00:00, 410.80it/s]\n",
            "        } [0.004s]\n",
            "      } [0.007s]\n",
            "      TokensMetric() {\n",
            "        Parallelizing computation on 1 items over 4 threads {\n",
            "\n",
            "100% 1/1 [00:00<00:00, 28728.11it/s]\n",
            "        } [0.0s]\n",
            "      } [0.0s]\n",
            "    } [0.008s]\n",
            "    Generated 91 stats.\n",
            "    Writing 1577 characters to benchmark_output/runs/v1/mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2/run_spec.json\n",
            "    Writing 295 characters to benchmark_output/runs/v1/mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2/scenario.json\n",
            "    Writing 4137 characters to benchmark_output/runs/v1/mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2/scenario_state.json\n",
            "    Writing 28352 characters to benchmark_output/runs/v1/mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2/stats.json\n",
            "    Writing 8375 characters to benchmark_output/runs/v1/mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2/per_instance_stats.json\n",
            "    CacheStats.print_status {\n",
            "      prod_env/cache/huggingface.sqlite: 3 queries, 1 computes\n",
            "    } [0.0s]\n",
            "  } [5.106s]\n",
            "100% 1/1 [00:05<00:00,  5.11s/it]\n",
            "  Symlinking benchmark_output/runs/v1 to latest.\n",
            "  Done.\n",
            "} [5.129s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!helm-summarize --suite v1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2nMIkB4Sz-4W",
        "outputId": "e0821e91-4af2-42e4-8b6e-13ec560841e5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-05-30 22:38:31.815590: I tensorflow/core/util/port.cc:110] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
            "2023-05-30 22:38:31.870466: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-05-30 22:38:32.792509: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "main {\n",
            "  Reading schema from schema.yaml...\n",
            "  Reading contamination information from contamination.yaml...\n",
            "  validate_contamination {\n",
            "  } [0.0s]\n",
            "100% 1/1 [00:00<00:00, 28.42it/s]\n",
            "  Summarizer.check_metrics_defined {\n",
            "    WARNING: metric name prefix_exact_match@5 undefined in schema.yaml but appears in 3 run specs, including mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2\n",
            "    WARNING: metric name quasi_prefix_exact_match@5 undefined in schema.yaml but appears in 3 run specs, including mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2\n",
            "  } [0.0s]\n",
            "  Summarizer.write_executive_summary {\n",
            "    Writing 43 characters to benchmark_output/runs/v1/summary.json\n",
            "  } [0.0s]\n",
            "  Writing 36568 characters to benchmark_output/runs/v1/runs.json\n",
            "  Writing 1699 characters to benchmark_output/runs/v1/run_specs.json\n",
            "  Writing 12385 characters to benchmark_output/runs/v1/groups.json\n",
            "  Writing 28421 characters to benchmark_output/runs/v1/groups_metadata.json\n",
            "  WARNING: run spec mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 does not have any stat matched by MetricNameMatcher(name='inference_denoised_runtime', split='test', sub_split=None, perturbation_name=None), 0 near misses matching just the name\n",
            "  Writing 262 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_accuracy.tex\n",
            "  Writing 10082 characters to benchmark_output/runs/v1/groups/json/core_scenarios_accuracy.json\n",
            "  Writing 281 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_calibration.tex\n",
            "  Writing 10805 characters to benchmark_output/runs/v1/groups/json/core_scenarios_calibration.json\n",
            "  Writing 279 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_robustness.tex\n",
            "  Writing 11460 characters to benchmark_output/runs/v1/groups/json/core_scenarios_robustness.json\n",
            "  Writing 273 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_fairness.tex\n",
            "  Writing 11340 characters to benchmark_output/runs/v1/groups/json/core_scenarios_fairness.json\n",
            "  Writing 250 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_efficiency.tex\n",
            "  Writing 12180 characters to benchmark_output/runs/v1/groups/json/core_scenarios_efficiency.json\n",
            "  Writing 429 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_general_information.tex\n",
            "  Writing 51956 characters to benchmark_output/runs/v1/groups/json/core_scenarios_general_information.json\n",
            "  Writing 238 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_bias.tex\n",
            "  Writing 45634 characters to benchmark_output/runs/v1/groups/json/core_scenarios_bias.json\n",
            "  Writing 246 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_toxicity.tex\n",
            "  Writing 8081 characters to benchmark_output/runs/v1/groups/json/core_scenarios_toxicity.json\n",
            "  Writing 272 characters to benchmark_output/runs/v1/groups/latex/core_scenarios_summarization_metrics.tex\n",
            "  Writing 11774 characters to benchmark_output/runs/v1/groups/json/core_scenarios_summarization_metrics.json\n",
            "  Writing 180848 characters to benchmark_output/runs/v1/groups/core_scenarios.json\n",
            "  WARNING: run spec mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 does not have any stat matched by MetricNameMatcher(name='inference_idealized_runtime', split='test', sub_split=None, perturbation_name=None), 0 near misses matching just the name\n",
            "  WARNING: run spec mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 does not have any stat matched by MetricNameMatcher(name='inference_denoised_runtime', split='test', sub_split=None, perturbation_name=None), 0 near misses matching just the name\n",
            "  Writing 274 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_accuracy.tex\n",
            "  Writing 14627 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_accuracy.json\n",
            "  Writing 441 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_general_information.tex\n",
            "  Writing 101474 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_general_information.json\n",
            "  Writing 293 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_calibration.tex\n",
            "  Writing 5123 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_calibration.json\n",
            "  Writing 291 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_robustness.tex\n",
            "  Writing 4745 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_robustness.json\n",
            "  Writing 285 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_fairness.tex\n",
            "  Writing 4697 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_fairness.json\n",
            "  Writing 250 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_bias.tex\n",
            "  Writing 27744 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_bias.json\n",
            "  Writing 258 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_toxicity.tex\n",
            "  Writing 5255 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_toxicity.json\n",
            "  Writing 266 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_apps_metrics.tex\n",
            "  Writing 2082 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_apps_metrics.json\n",
            "  Writing 276 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_copyright_metrics.tex\n",
            "  Writing 4539 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_copyright_metrics.json\n",
            "  Writing 286 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_disinformation_metrics.tex\n",
            "  Writing 3292 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_disinformation_metrics.json\n",
            "  Writing 264 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_bbq_metrics.tex\n",
            "  Writing 2134 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_bbq_metrics.json\n",
            "  Writing 339 characters to benchmark_output/runs/v1/groups/latex/targeted_evaluations_efficiency_detailed.tex\n",
            "  Writing 100669 characters to benchmark_output/runs/v1/groups/json/targeted_evaluations_efficiency_detailed.json\n",
            "  Writing 288919 characters to benchmark_output/runs/v1/groups/targeted_evaluations.json\n",
            "  WARNING: run spec mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 does not have any stat matched by MetricNameMatcher(name='inference_denoised_runtime', split='test', sub_split=None, perturbation_name=None), 0 near misses matching just the name\n",
            "  Writing 270 characters to benchmark_output/runs/v1/groups/latex/question_answering_accuracy.tex\n",
            "  Writing 6074 characters to benchmark_output/runs/v1/groups/json/question_answering_accuracy.json\n",
            "  Writing 289 characters to benchmark_output/runs/v1/groups/latex/question_answering_calibration.tex\n",
            "  Writing 8385 characters to benchmark_output/runs/v1/groups/json/question_answering_calibration.json\n",
            "  Writing 287 characters to benchmark_output/runs/v1/groups/latex/question_answering_robustness.tex\n",
            "  Writing 7720 characters to benchmark_output/runs/v1/groups/json/question_answering_robustness.json\n",
            "  Writing 281 characters to benchmark_output/runs/v1/groups/latex/question_answering_fairness.tex\n",
            "  Writing 7640 characters to benchmark_output/runs/v1/groups/json/question_answering_fairness.json\n",
            "  Writing 258 characters to benchmark_output/runs/v1/groups/latex/question_answering_efficiency.tex\n",
            "  Writing 7253 characters to benchmark_output/runs/v1/groups/json/question_answering_efficiency.json\n",
            "  Writing 437 characters to benchmark_output/runs/v1/groups/latex/question_answering_general_information.tex\n",
            "  Writing 29942 characters to benchmark_output/runs/v1/groups/json/question_answering_general_information.json\n",
            "  Writing 246 characters to benchmark_output/runs/v1/groups/latex/question_answering_bias.tex\n",
            "  Writing 19700 characters to benchmark_output/runs/v1/groups/json/question_answering_bias.json\n",
            "  Writing 254 characters to benchmark_output/runs/v1/groups/latex/question_answering_toxicity.tex\n",
            "  Writing 3896 characters to benchmark_output/runs/v1/groups/json/question_answering_toxicity.json\n",
            "  Writing 94724 characters to benchmark_output/runs/v1/groups/question_answering.json\n",
            "  WARNING: run spec mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 does not have any stat matched by MetricNameMatcher(name='inference_denoised_runtime', split='test', sub_split=None, perturbation_name=None), 0 near misses matching just the name\n",
            "  Writing 252 characters to benchmark_output/runs/v1/groups/latex/knowledge_accuracy.tex\n",
            "  Writing 4374 characters to benchmark_output/runs/v1/groups/json/knowledge_accuracy.json\n",
            "  Writing 271 characters to benchmark_output/runs/v1/groups/latex/knowledge_calibration.tex\n",
            "  Writing 5101 characters to benchmark_output/runs/v1/groups/json/knowledge_calibration.json\n",
            "  Writing 269 characters to benchmark_output/runs/v1/groups/latex/knowledge_robustness.tex\n",
            "  Writing 4723 characters to benchmark_output/runs/v1/groups/json/knowledge_robustness.json\n",
            "  Writing 263 characters to benchmark_output/runs/v1/groups/latex/knowledge_fairness.tex\n",
            "  Writing 4675 characters to benchmark_output/runs/v1/groups/json/knowledge_fairness.json\n",
            "  Writing 228 characters to benchmark_output/runs/v1/groups/latex/knowledge_bias.tex\n",
            "  Writing 4946 characters to benchmark_output/runs/v1/groups/json/knowledge_bias.json\n",
            "  Writing 236 characters to benchmark_output/runs/v1/groups/latex/knowledge_toxicity.tex\n",
            "  Writing 1504 characters to benchmark_output/runs/v1/groups/json/knowledge_toxicity.json\n",
            "  Writing 240 characters to benchmark_output/runs/v1/groups/latex/knowledge_efficiency.tex\n",
            "  Writing 5124 characters to benchmark_output/runs/v1/groups/json/knowledge_efficiency.json\n",
            "  Writing 419 characters to benchmark_output/runs/v1/groups/latex/knowledge_general_information.tex\n",
            "  Writing 20510 characters to benchmark_output/runs/v1/groups/json/knowledge_general_information.json\n",
            "  Writing 53523 characters to benchmark_output/runs/v1/groups/knowledge.json\n",
            "  WARNING: run spec mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 does not have any stat matched by MetricNameMatcher(name='platt_coef', split='test', sub_split=None, perturbation_name=None), 0 near misses matching just the name\n",
            "  WARNING: run spec mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 does not have any stat matched by MetricNameMatcher(name='platt_intercept', split='test', sub_split=None, perturbation_name=None), 0 near misses matching just the name\n",
            "  WARNING: metric name none undefined in schema.yaml, skipping\n",
            "  Writing 472 characters to benchmark_output/runs/v1/groups/latex/calibration_calibration_detailed.tex\n",
            "  Writing 24753 characters to benchmark_output/runs/v1/groups/json/calibration_calibration_detailed.json\n",
            "  Writing 256 characters to benchmark_output/runs/v1/groups/latex/calibration_accuracy.tex\n",
            "  Writing 3199 characters to benchmark_output/runs/v1/groups/json/calibration_accuracy.json\n",
            "  Writing 29302 characters to benchmark_output/runs/v1/groups/calibration.json\n",
            "  WARNING: run spec mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2 does not have any stat matched by MetricNameMatcher(name='inference_denoised_runtime', split='test', sub_split=None, perturbation_name=None), 0 near misses matching just the name\n",
            "  Writing 448 characters to benchmark_output/runs/v1/groups/latex/mmlu_mmlu_subject:philosophy.tex\n",
            "  Writing 8389 characters to benchmark_output/runs/v1/groups/json/mmlu_mmlu_subject:philosophy.json\n",
            "  Writing 8811 characters to benchmark_output/runs/v1/groups/mmlu.json\n",
            "  Summarizer.write_cost_report {\n",
            "    Writing 162 characters to benchmark_output/runs/v1/costs.json\n",
            "  } [0.0s]\n",
            "  Parallelizing computation on 1 items over 8 threads {\n",
            "    write_run_display_json {\n",
            "  0% 0/1 [00:00<?, ?it/s]      Writing 621 characters to benchmark_output/runs/v1/mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2/instances.json\n",
            "      Writing 351 characters to benchmark_output/runs/v1/mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2/display_predictions.json\n",
            "      Writing 1708 characters to benchmark_output/runs/v1/mmlu:subject=philosophy,method=multiple_choice_joint,model=huggingface_gpt2/display_requests.json\n",
            "    } [0.023s]\n",
            "100% 1/1 [00:00<00:00, 42.34it/s]\n",
            "  } [0.024s]\n",
            "  Done.\n",
            "} [1.453s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can now check out the results in the `benchmark_output` folder!"
      ],
      "metadata": {
        "id": "-vu7kWwK5aMm"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Assignment Part 3:\n",
        "\n",
        "Try this process out on any Hugging Face model (maybe `BLOOMz`, as an example) and use an alternate suite or metric to examine!\n",
        "\n",
        "Refer to the [docs](https://crfm-helm.readthedocs.io/en/latest/) for a comprehensive overview of the available options!\n",
        "\n"
      ],
      "metadata": {
        "id": "23kU_nPO5pRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "### YOUR CODE HERE"
      ],
      "metadata": {
        "id": "a4C7shhp6Q8u"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}